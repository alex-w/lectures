\documentclass[10pt,pdf,hyperref={unicode}]{beamer}
\hypersetup{pdfpagemode=FullScreen}
\usepackage{lect}

\title[Компьютерная обработка. Лекция 2]{Компьютерная обработка результатов измерений}
\subtitle{Лекция 2. Статистика и вероятность. Случайные величины и распределения}
\date{12 июля 2016 года}

\begin{document}
% Титул
\begin{frame}
\maketitle
\end{frame}
% Содержание
\begin{frame}
\tableofcontents
\end{frame}

\section{Случайные величины, вероятность}
\begin{frame}{Случайные величины, вероятность}
\begin{defin}
\ж Случайной величиной\н называется величина~$X$, если все ее возможные значения образуют конечную 
или бесконечную последовательность чисел~$x_1,\ldots$, $x_N$, и если принятие ею каждого из этих 
значений есть случайное событие.
\end{defin}
\begin{defin}\begin{columns}\column{0.75\textwidth}
\ж Вероятность\н наступления данного события~--- это предел относительной частоты наступления 
данного события~$n_k/N$:\column{0.25\textwidth}
$$P(x_k)=\lim_{N\to\infty}\frac{n_k}{N}.$$
\end{columns}
\end{defin}
\begin{block}{}
Совместные и несовместные события, полная группа, свойства вероятности. 
Для непрерывных случайных величин вводят понятие\ж плотности вероятности\н:
$$\rho(x)=\lim_{\Delta x\to 0}\frac{P(x<X<x+\Delta x)}{\Delta
x}=\frac{dP}{dx}.$$
$$P(x_1<X<x_2)=\Int_{x_1}^{x_2}\rho(x)\,dx.$$
\end{block}
\end{frame}


\section{Характеристики случайных величин}
\begin{frame}{Характеристики случайных величин}
\begin{block}{Независимые случайные величины}
$P(x_ny_n)=P(x_n)P(y_n)$.
\end{block}
\begin{block}{Среднее арифметическое и математическое ожидание}
$$\aver{X}=\frc1{N}\sum_{n=1}^N x_n,$$
$$M(X)\equiv\mean{X}=\lim_{N\to\infty}\frac1{N}\sum_{n=1}^N x_n\quad\text{и}\quad
M(X)=\Infint x\phi(x)\,dx.$$
\end{block}
\end{frame}


\begin{frame}{}
\begin{block}{Свойства математического ожидания}
\begin{itemize}\setlength{\itemsep}{2pt}
\item $\mean\const=\const$;
\item $\mean{\sum\C_n\cdot X_n}=\sum\C_n\cdot \mean{X_n}$,
где $\C_n$~-- постоянная величина; 
\item $\mean{\prod X_n}=\prod \mean{X_n}$ (для независимых случайных величин);
\item $\mean{f(x)}=\Infint f(x)\phi(x)\,dx$ (для непрерывных случайных величин).
\end{itemize}
\end{block}
\begin{block}{$\mean{X}\Leftrightarrow\aver{X}$? Закон больших чисел}
Неравенство Чебыш\"ева: $P(|X-\mean{X}|<\epsilon)\ge 1-D(X)/\epsilon^2\quad\Rightarrow$
$$\lim_{n\to\infty} P\Bigl(\Bigl|\frac{\sum
X_n}{n}-\frac{\sum\mean{X_n}}{n}\Bigr|<\epsilon\Bigr)=1.$$
Теорема Бернулли: $\lim\limits_{n\to\infty} P(|m/n-p|<\epsilon)=1$.
\end{block}
\end{frame}

\begin{frame}{Характеристические значения распределений}
\only<1>{\begin{block}{Медиана и мода}
{\ж Мода}~--- наиболее часто встречающееся значение (но вполне могут быть мультимодальные 
распределения). {\ж Медиана} делит площадь распределения пополам.
\end{block}
\img[0.6]{mode_median}}
\only<2>{\begin{block}{Моменты}
Если $f(x)=(x-x_0)^n$, то $\mean{f(x)}$~--- момент порядка~$n$. Если $x_0=0$~--- начальный
момент, если $x_0=\mean{X}$--- центральный момент.

Моменты нулевого порядка равны~1, начальный момент первого порядка равен математическому ожиданию 
случайной величины; центральный момент первого порядка равен нулю.

Центральный момент второго порядка называют\ж дисперсией\н: $D(X)=\mean{(x-\mean{x})^2}\equiv 
\mean{x^2}-\mean{x}^2$,  $\sigma=\sqrt{D}$.

\smallskip

Свойства дисперсии:
\begin{itemize}
\item $D(\const)=0$;
\item $D(\const X)=C^2 D(X)$, где $\C$~-- постоянная величина;
\item $D(\sum X_n)=\sum D(X_n)$.
\end{itemize}
\end{block}
}
\end{frame}

\section{Законы распределения}
\begin{frame}{Законы распределения}
\begin{defin}
\ж Закон распределения\н \к дискретной\н случайной величины~--- соответствие между возможными 
значениями и их вероятностями.
\end{defin}
\begin{block}{Функция распределения}
$$F(x)\equiv P(X\le x)=\Int_{-\infty}^x\phi(x)\,dx, \qquad
\Infint\phi(x)\,dx=1.$$
$$P(a\le X\le b)=F(b)-F(a).$$
\end{block}
\end{frame}

\begin{frame}{Равномерное распределение}
\begin{columns}\column{0.45\textwidth}
\begin{block}{}
$$
\phi(x)=\begin{cases}\frac1{b-a}, & x\in [a,b] \\ 0, & x\not\in [a,b]
\end{cases}.
$$
$$F(x)= \begin{cases} 0, & x < a \\ \frac{x-a}{b-a}, & a \le x < b \\ 1, & x \ge
b \end{cases}.
$$
\end{block}\column{0.45\textwidth}
\begin{block}{}
$\mean{X}=\med(X)=(a+b)/2$, $\moda(X)=\forall x\in[a,b]$,
$\displaystyle\sigma^2_X = \frac{(b-a)^2}{12}$.
\end{block}
\end{columns}

\smimg[0.45]{Uniform_distribution_PDF}\hspace{3pt}
\smimg[0.45]{Uniform_distribution_CDF}
\end{frame}

\begin{lightframe}{Биномиальное распределение}
\vspace*{-0.8em}\begin{block}{}
\ж Формула Бернулли\н:
$\displaystyle P_n(k)=C_n^k p^k q^{n-k},\quad C_n^k=\frac{n!}{k!(n-k)!},\quad
q=1-p.$
$$(p+q)^n=C_n^n p^n+\cdots+C_n^k p^k q^{n-k}+\cdots+C_n^0 q^k.$$
Описывает вероятность наступления события~$k$
раз в~$n$ независимых испытаниях
\end{block}\vspace*{-1em}
\begin{columns}
\column{0.45\textwidth}
\img{Binomial_Distribution}
\column{0.55\textwidth}
\begin{block}{}
$$
F(k;n,p)=P(X\leq k)=\sum_{i=0}^\floor{k} C_n^i p^i(1-p)^{n-i}.$$
$\mean{X}=np$, $\moda(X)=\floor{(n+1)p}$, $\floor{np}\le\med(X)\le\ceil{np}$,
$\sigma^2_X = npq$.
\end{block}
\end{columns}
\end{lightframe}

\begin{frame}{Распределение Пуассона}
\vspace*{-2em}\begin{block}{}
При $n\to\infty$ распределение Бернулли преобразуется в распределение Пуассона ($\lambda=np$):
$$P_n(k)=\frac{\lambda^k}{k!}\exp(-\lambda).$$
\end{block}
\begin{columns}\column{0.48\textwidth}
\begin{block}{}
$F(k, \lambda) = \displaystyle\frac{\Gamma(k+1,\lambda)}{k!}$,
$\mean{X} = \lambda$, $\moda(X) = \floor{\lambda}$, 
$\med{X}\approx\floor{\lambda+1/3-0.02/\lambda}$,
$\sigma^2_X = \lambda$.

С ростом $\lambda$ распределение Пуассона стремится к распределению Гаусса.
\end{block}
\column{0.48\textwidth}
\img{poissonpdf}
\end{columns}
\end{frame}

\begin{frame}{Распределение Гаусса}
\vspace*{-2em}\begin{block}{}
$$
\phi (x) = \frac 1 {\sigma \sqrt {2 \pi}} \exp \left( -\frac {(x -\mean{x})^2}{2
\sigma^2} \right)
$$
\end{block}
\vspace*{-1em}\begin{block}{}
$F(x) = \displaystyle\frac 1{\sigma \sqrt {2 \pi}} \Int_{-\infty}^x \exp \left( -\frac{(t
-\mean{x})^2}{2 \sigma^2} \right)\, dt$,
$\moda(X) = \med{X} = \mean{X}$.
\end{block}
\vspace*{-1em}\img[0.6]{normpdf}
\end{frame}

\begin{frame}{Показательное (экспоненциальное) распределение}
\vspace*{-1em}\begin{block}{}
Время между двумя последовательными свершениями события
$$f(x)=\begin{cases}
0,& x<0,\\
\lambda\exp(-\lambda x),& x\ge0;
\end{cases}\qquad
F(x)=\begin{cases}
0,& x<0,\\
1-\exp(-\lambda x),& x\ge0,
\end{cases}
$$
\end{block}
\vspace*{-1em}\begin{block}{}
$\mean{X} = \lambda^{-1}$,
$\moda(X) = 0$, $\med{X} = \ln(2)/\lambda$,
$\sigma^2_X = \lambda^{-2}$.
\end{block}
\vspace*{-1em}\img[0.5]{exppdf}
\end{frame}

\section{Корреляция и ковариация}
\begin{frame}{Корреляция и ковариация}
\begin{defin}
\ж{}Ковариация\н является мерой линейной зависимости случайных величин и определяется формулой:
$\mathrm{cov}(X,Y)=\mean{(X-\mean{X})(Y-\mean{Y})}$ $\Longrightarrow$ $\mathrm{cov}(X,X) = 
\sigma^2_X$.
\к Ковариация независимых случайных величин равна нулю\н, обратное неверно.
\end{defin}
\begin{block}{}
Если ковариация положительна, то с ростом значений одной случайной величины, значения второй имеют 
тенденцию возрастать, а если знак отрицательный~--- убывать.

Масштаб зависимости величин пропорционален их дисперсиям $\Longrightarrow$ масштаб можно 
отнормировать (\ж{}коэффициент корреляции\н Пирсона):
$$\rho_{X,Y}=\frac{\mathrm{cov}(X,Y)}{\sigma X\sigma Y}, \qquad \mathbf{r}\in[-1,1].$$
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
Коэффициент корреляции равен~$\pm1$ тогда и только тогда, когда~$X$ и~$Y$ линейно зависимы. Если 
они независимы, $\rho_{X,Y}=0$ (\ж{}обратное неверно!\н). Промежуточные значения коэффициента 
корреляции не позволяют однозначно судить о зависимости случайных величин, но позволяет предполагать 
степень их зависимости.
\end{block}
\begin{block}{Корреляционная функция}
Одна из разновидностей~---\ж автокорреляционная функция\н:
$$
\Psi(\tau) = \Int f(t) f(t-\tau)\, dt\equiv
\Int f(t+\tau) f(t)\,dt.
$$
Для дискретных случайных величин автокорреляционная функция имеет вид
$$
\Psi(\tau) = \aver{X(t)X(t-\tau)}\equiv\aver{X(t+\tau)X(t)}.
$$
\end{block}
\end{frame}

\begin{blueframe}{}
\begin{block}{Взаимно корреляционная функция}
Другая разновидность~---\ж кросс--корреляционная функция\н:
$$(f\star g)(t)\stackrel{\mathrm{def}}{=}\Infint f^{*}(\tau)g(t+\tau)\,d\tau$$
свертка:
$$ (f*g)(x)\stackrel{\mathrm{def}}{=}\Infint f(y)\,g(x-y)\,dy = \Infint f(x-y)\,g(y)\, dy.$$

\end{block}
\img[0.5]{convcorr}
\end{blueframe}

\begin{frame}{}
\begin{block}{}
Если $X$ и $Y$~--- два независимых случайных числа с функциями распределения вероятностей 
$f$ и $g$, то $f\star g$ соответствует распределению вероятностей выражения $-X+Y$, а $f*g$~---
распределению вероятностей суммы $X + Y$.

ВКФ часто используется для поиска в длинной последовательности более короткой заранее известной, 
определения сдвига (см.~рис).

Связь со сверткой: $f(t)\star g(t) = f^*(-t) * g(t)$, если $f$ и $g$ четны, то 
$f(t)\star g(t) = f(t) * g(t)$. Через преобразование Фурье:
$\FT{f \star g} = \FT{f}^*\cdot\FT{g}$.
\end{block}\img[0.6]{autocorr}
\end{frame}

\section{Шум}
\begin{frame}{Шум}
\begin{defin}
\ж Шум\н~--- беспорядочные колебания различной физической природы, отличающиеся сложной временной и
спектральной структурой.
\end{defin}
\begin{block}{}
\ж Белый шум\н, $\xi(t)$, имеет время корреляции много меньше всех характерных времен физической 
системы;  $\mean{\xi(t)}=0$, $\Psi(t,\tau)=\aver{\xi(t+\tau)\xi(t)}=\sigma^2(t)\delta(\tau)$.
Разновидность~--- AWGN.

\ж Дробовой шум\н имеет пуассонову статистику~\so $\sigma_X\propto\sqrt{x}$ и 
$\SNR(N)\propto\sqrt{N}$. Суточные и вековые корреляции.

Шум вида \ж<<соль--перец>>\н обычно характерен для изображений, считываемых с ПЗС.
\end{block}
\end{frame}
\begin{frame}{SNR}
\begin{defin}
\ж SNR\н~--- безразмерная величина, равная отношению мощности полезного сигнала к мощности шума.
\end{defin}
\begin{block}{}
$$\SNR = {P_\mathrm{signal} \over P_\mathrm{noise}} = \left ({A_\mathrm{signal} \over 
A_\mathrm{noise} } \right )^2, \quad
\SNR(dB) = 10 \lg \left ( {P_\mathrm{signal} \over P_\mathrm{noise}}
\right )
= 20 \lg \left ( {A_\mathrm{signal} \over A_\mathrm{noise}} \right ).
$$
\end{block}

\img[0.6]{SNR}
\centerline{\tiny (10, 0, -10~дБ.)}
\end{frame}


\begin{frame}{Спасибо за внимание!}
\centering
\begin{minipage}{5cm}
\begin{block}{mailto}
eddy@sao.ru\\
edward.emelianoff@gmail.com
\end{block}\end{minipage}
\end{frame}
\end{document}
